{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CgIMTHCUzbUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd180f9e-3e5c-4a51-db34-f5172e950948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "path = '/content/drive/MyDrive/DeepOmics/Project1/Proposed/Simulations/Linear/S1/'\n",
        "X1Train = genfromtxt(path + 'X1Train.csv', delimiter=',')\n",
        "X1Test = genfromtxt(path + 'X1Test.csv', delimiter=',')\n",
        "X1Tune = genfromtxt(path + 'X1Tune.csv', delimiter=',')\n",
        "X2Train = genfromtxt(path + 'X2Train.csv', delimiter=',')\n",
        "X2Test = genfromtxt(path + 'X2Test.csv', delimiter=',')\n",
        "X2Tune = genfromtxt(path + 'X2Tune.csv', delimiter=',')\n",
        "YTrain = genfromtxt(path + 'YTrain.csv', delimiter=',')\n",
        "YTest = genfromtxt(path + 'YTest.csv', delimiter=',')\n",
        "YTune = genfromtxt(path + 'YTune.csv', delimiter=',')\n",
        "\n",
        "print(X1Train.shape)\n",
        "print(X1Test.shape)\n",
        "print(X1Tune.shape)\n",
        "print(X2Train.shape)\n",
        "print(X2Test.shape)\n",
        "print(X2Tune.shape)\n",
        "print(YTrain.shape)\n",
        "print(YTest.shape)\n",
        "print(YTune.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQlqXBr9zljc",
        "outputId": "fa2b9123-1fca-40e1-a7c5-a40ce1018f9d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 540)\n",
            "(20000, 1080)\n",
            "(20000, 540)\n",
            "(20000, 540)\n",
            "(20000, 1080)\n",
            "(20000, 540)\n",
            "(20, 540)\n",
            "(20, 1080)\n",
            "(20, 540)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X must be the stacks of n*p matrixes from each view where n means the number of sampels and p implies\n",
        "# the number of features in that view. Notice, p may vary in different views but n must be the same\n",
        "# cross all the views\n",
        "\n",
        "# y must be the array of labels of the n samples\n",
        "\n",
        "block = int(X1Train.shape[0] / 20)\n",
        "i = 0\n",
        "X_train = [np.transpose(X1Train[block*i:block*(i+1),:]), np.transpose(X2Train[block*i:block*(i+1),:])]\n",
        "X_tune = [np.transpose(X1Tune[block*i:block*(i+1),:]), np.transpose(X2Tune[block*i:block*(i+1),:])]\n",
        "X_test = [np.transpose(X1Test[block*i:block*(i+1),:]), np.transpose(X2Test[block*i:block*(i+1),:])]\n",
        "y_train = YTrain[i,:]\n",
        "y_tune = YTune[i,:]\n",
        "y_test = YTest[i,:]\n",
        "\n",
        "print(np.array(X_train).shape)\n",
        "print(np.array(X_tune).shape)\n",
        "print(np.array(X_test).shape)\n",
        "print(np.array(y_train).shape)\n",
        "print(np.array(y_tune).shape)\n",
        "print(np.array(y_test).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrLNQfK7qlPF",
        "outputId": "25c2fa7c-254a-4969-e299-16b033685531"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 540, 1000)\n",
            "(2, 540, 1000)\n",
            "(2, 1080, 1000)\n",
            "(540,)\n",
            "(540,)\n",
            "(1080,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import random\n",
        "from sklearn import svm\n",
        "import itertools\n",
        "import os\n",
        "import copy\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# set random seed\n",
        "def torch_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "# normalize training or testing data\n",
        "def myNormalize(input, train=None):\n",
        "    data = copy.copy(input)\n",
        "    h, w = data.shape[0], data.shape[1]\n",
        "    if train is None:\n",
        "        for i in range(w):\n",
        "            tmp_mean = np.mean(data[:, i])\n",
        "            tmp_std = np.std(data[:, i])\n",
        "            data[:, i] = (data[:, i] - tmp_mean) / tmp_std\n",
        "    else:\n",
        "        assert w == train.shape[1], 'Testing data should have the same number of features as the training data!'\n",
        "        for i in range(w):\n",
        "            tmp_mean = np.mean(train[:, i])\n",
        "            tmp_std = np.std(train[:, i])\n",
        "            data[:, i] = (data[:, i] - tmp_mean) / tmp_std\n",
        "    return data"
      ],
      "metadata": {
        "id": "rZJvUYCqMvLO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(X_train, y_train, comb, X_tune=None, y_tune=None, important=None, top_rate=0.1, edged=None, vWeightd=None, l1=0.1,\n",
        "        epochs=1000, plot=False, train=False, device='cpu'):\n",
        "    # hyper-parameters\n",
        "    K = comb[0]\n",
        "    lr_net = comb[1]\n",
        "    lr_z = comb[2]\n",
        "    C = comb[3]\n",
        "\n",
        "    # data dimension\n",
        "    D = len(X_train)\n",
        "    n = X_train[0].shape[0]\n",
        "\n",
        "    # normalized training\n",
        "    X_train_normalized = []\n",
        "\n",
        "    # numbers of important features in each view\n",
        "    gt = []\n",
        "\n",
        "    # numbers of features in each view\n",
        "    p = []\n",
        "    for i in range(D):\n",
        "        X_train_normalized.append(torch.tensor(myNormalize(X_train[i])).to(device))\n",
        "        p.append(X_train[i].shape[1])\n",
        "        if important != None:\n",
        "            gt.append(len(important[i]))\n",
        "        else:\n",
        "            gt.append(int(X_train[i].shape[1] * top_rate))\n",
        "\n",
        "    if edged != None:\n",
        "        lap = normalized_Laplacian(X_train_normalized, edged, vWeightd)\n",
        "\n",
        "    def norm_21(x):\n",
        "        return torch.norm(x, dim=0).sum()\n",
        "\n",
        "    def unsupervised_loss_1(X, Z_pred, model):\n",
        "        loss = 0\n",
        "        if edged != None:\n",
        "            for i in range(D):\n",
        "                loss += norm_21(X[i] - model[i](Z_pred)) + l1 * norm_21(model[i](Z_pred) @ lap[i].to(device))\n",
        "        else:\n",
        "            for i in range(D):\n",
        "                loss += norm_21(X[i] - model[i](Z_pred)) + l1 * norm_21(model[i](Z_pred))\n",
        "        return loss / X[0].shape[0]\n",
        "\n",
        "    def unsupervised_loss_2(X, Z_pred, model):\n",
        "        loss = 0\n",
        "        for i in range(D):\n",
        "            loss += torch.norm(X[i] - model[i](Z_pred)) ** 2\n",
        "        return loss / X[0].shape[0]\n",
        "\n",
        "    unsupervised_history_1 = []\n",
        "    unsupervised_history_2 = []\n",
        "\n",
        "    # first-stage reconstruction using data with all features\n",
        "    # initial Z, NNs\n",
        "    Z_1 = torch.randn(n, K, requires_grad=True, device=device)\n",
        "    model_1 = list(range(D))\n",
        "    optimizer_1 = list(range(D))\n",
        "    for i in range(D):\n",
        "        model_1[i] = torch.nn.Sequential(\n",
        "            torch.nn.Linear(K, 64),\n",
        "            torch.nn.GroupNorm(16, 64),\n",
        "            torch.nn.ELU(),\n",
        "            torch.nn.Linear(64, 256),\n",
        "            torch.nn.GroupNorm(64, 256),\n",
        "            torch.nn.ELU(),\n",
        "            torch.nn.Linear(256, p[i]),\n",
        "        ).to(device)\n",
        "        optimizer_1[i] = torch.optim.Adam(model_1[i].parameters(), lr_net)\n",
        "    optimizer_z_1 = torch.optim.Adam([Z_1], lr_z)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        unsupervised = unsupervised_loss_1(X_train_normalized, Z_1, model_1)\n",
        "        unsupervised_history_1.append(unsupervised.item())\n",
        "        for i in range(D):\n",
        "            optimizer_1[i].zero_grad()\n",
        "        optimizer_z_1.zero_grad()\n",
        "        unsupervised.backward()\n",
        "        for i in range(D):\n",
        "            optimizer_1[i].step()\n",
        "        optimizer_z_1.step()\n",
        "\n",
        "    if plot:\n",
        "        plt.plot(unsupervised_history_1)\n",
        "        plt.title(\"Unsupervised History 1\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    # feature selection\n",
        "    features = []\n",
        "    X_train_fs = []\n",
        "    \n",
        "    for d in range(D):\n",
        "        pre = abs(model_1[d](Z_1))\n",
        "        norm_pre = torch.norm(pre, dim=0)\n",
        "        features_pre = (sorted(range(len(norm_pre)), key=lambda i: norm_pre[i]))[::-1]\n",
        "        features_pre = features_pre[:gt[d]]\n",
        "        features_pre = sorted(features_pre)\n",
        "        features.append(features_pre)\n",
        "        X_train_fs.append(X_train_normalized[d][:, features_pre])\n",
        "\n",
        "    #evaluate chosen features if we know the ground truth of important features\n",
        "    features_metrics = []\n",
        "    if len(important) > 0:\n",
        "        for d in range(D):\n",
        "            pre = abs(model_1[d](Z_1))\n",
        "            norm_pre = torch.norm(pre, dim=0)\n",
        "            features_pre = (sorted(range(len(norm_pre)), key=lambda i: norm_pre[i]))[::-1]\n",
        "            features_pre = features_pre[:gt[d]]\n",
        "            features_pre = sorted(features_pre)\n",
        "            all_features = range(X_train[d].shape[1])\n",
        "\n",
        "            tp = len(set(important[d]) & set(features_pre))\n",
        "            fp = len(set(features_pre) - set(important[d]))\n",
        "            fn = len(set(important[d]) - set(features_pre))\n",
        "            tn = len((set(all_features) - set(important[d])) & (set(all_features) - set(features_pre)))\n",
        "            tpr = tp / (tp + fn)\n",
        "            fpr = fp / (fp + tn)\n",
        "            precision = tp / (tp + fp)\n",
        "            if precision + tpr == 0:\n",
        "                f_score = 0\n",
        "            else:\n",
        "                f_score = 2 * precision * tpr / (precision + tpr)\n",
        "            features_metrics.append([tpr, fpr, f_score])\n",
        "    \n",
        "    # second-stage reconstruction using data without unimportant features\n",
        "    model_2 = list(range(D))\n",
        "    optimizer_2 = list(range(D))\n",
        "    for i in range(D):\n",
        "        model_2[i] = torch.nn.Sequential(\n",
        "            torch.nn.Linear(K, 64),\n",
        "            torch.nn.GroupNorm(16, 64),\n",
        "            torch.nn.ELU(),\n",
        "            torch.nn.Linear(64, 256),\n",
        "            torch.nn.GroupNorm(64, 256),\n",
        "            torch.nn.ELU(),\n",
        "            torch.nn.Linear(256, gt[i]),\n",
        "        ).to(device)\n",
        "        optimizer_2[i] = torch.optim.Adam(model_2[i].parameters(), lr_net)\n",
        "    Z_2 = torch.randn(n, K, requires_grad=True, device=device)\n",
        "    optimizer_z_2 = torch.optim.Adam([Z_2], lr_z)\n",
        "    for epoch in range(epochs):\n",
        "        unsupervised = unsupervised_loss_2(X_train_fs, Z_2, model_2)\n",
        "        unsupervised_history_2.append(unsupervised.item())\n",
        "        for i in range(D):\n",
        "            optimizer_2[i].zero_grad()\n",
        "        optimizer_z_2.zero_grad()\n",
        "        unsupervised.backward()\n",
        "        for i in range(D):\n",
        "            optimizer_2[i].step()\n",
        "        optimizer_z_2.step()\n",
        "        # projected gradient descent (PGD)\n",
        "        with torch.no_grad():\n",
        "            tmp_norm = torch.norm(Z_2, dim=1).reshape(-1, 1)\n",
        "            tmp_norm = torch.clamp(tmp_norm, min = 1)\n",
        "            Z_2.data = Z_2.data / tmp_norm\n",
        "    if plot:\n",
        "        plt.plot(unsupervised_history_2)\n",
        "        plt.title(\"Unsupervised History 2\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    # train a SVM classifier using learned latent code Z of training data\n",
        "    clf = svm.SVC(kernel='rbf', C=C, gamma='scale')\n",
        "    if device != 'cpu':\n",
        "        clf.fit(Z_2.cpu().detach().numpy(), y_train)\n",
        "    else:\n",
        "        clf.fit(Z_2.detach().numpy(), y_train)\n",
        "\n",
        "    # return important features, DNNs of each view, classifier and evaluation of selected features\n",
        "    # if no tuning data\n",
        "    if X_tune == None:\n",
        "        return features, model_2, clf, features_metrics\n",
        "\n",
        "\n",
        "    # testing stage\n",
        "    n_tune = X_tune[0].shape[0]\n",
        "\n",
        "    # normalized tuning data\n",
        "    X_tune_normalized = []\n",
        "    for i in range(D):\n",
        "        X_tune_normalized.append(torch.tensor(myNormalize(X_tune[i], X_train[i])).to(device))\n",
        "\n",
        "    # normalized tuning data without unimportant features\n",
        "    X_tune_fs = []\n",
        "    for d in range(D):\n",
        "        X_tune_fs.append(X_tune_normalized[d][:, features_pre])\n",
        "\n",
        "    # get Z from testing data\n",
        "    Z_tune = torch.randn(n_tune, K, requires_grad=True, device=device)\n",
        "    optimizer_z_tune = torch.optim.Adam([Z_tune], lr_z)\n",
        "    Z_history = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "        loss_z = unsupervised_loss_2(X_tune_fs, Z_tune, model_2)\n",
        "        Z_history.append(loss_z.item())\n",
        "        optimizer_z_tune.zero_grad()\n",
        "        loss_z.backward()\n",
        "        optimizer_z_tune.step()\n",
        "        # projected gradient descent (PGD)\n",
        "        with torch.no_grad():\n",
        "            tmp_norm = torch.norm(Z_tune, dim=1).reshape(-1, 1)\n",
        "            tmp_norm = torch.clamp(tmp_norm, min = 1)\n",
        "            Z_tune.data = Z_tune.data / tmp_norm\n",
        "    if plot:\n",
        "        plt.plot(Z_history)\n",
        "        plt.title(\"Z History\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    # pass the learned Z of testing data to the trained classifer to do prediction\n",
        "    if device != 'cpu':\n",
        "        prediction = clf.predict(Z_tune.cpu().detach().numpy())\n",
        "    else:\n",
        "        prediction = clf.predict(Z_tune.detach().numpy())\n",
        "    acc = (prediction.reshape(-1) == y_tune).sum().item() / n_tune\n",
        "    # return accuracy and evaluation of selected features\n",
        "    return acc, features_metrics"
      ],
      "metadata": {
        "id": "Dry3Xd2aswab"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X_train, y_train, X_tune=None, y_tune=None, comb_num=0, best_comb=None, important=None, top_rate=0.1, edged=None, vWeightd=None, l1=0.1,\n",
        "        epochs=1000, fold=5, plot=False, verbose=True, gpu=False):\n",
        "    \n",
        "    # choose to use GPU or CPU\n",
        "    if gpu == True:\n",
        "        torch.backends.cudnn.enabled = True\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        device = 'cuda'\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "        if verbose:\n",
        "            print(\"Training using GPU\",flush=True)\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "        if verbose:\n",
        "            print(\"Training using CPU\",flush=True)\n",
        "\n",
        "    # upper bound of the dimension of the latent code K\n",
        "    if len(important) > 0:\n",
        "        feature_num = [len(x) for x in important]\n",
        "        max_K = min(feature_num)\n",
        "    else:\n",
        "        max_K = int(min(X_train[0].shape[1], X_train[1].shape[1]) * top_rate)\n",
        "\n",
        "    # search the best hyper-parameter combination\n",
        "    if best_comb == None:\n",
        "        #default combination\n",
        "        if comb_num == 0:\n",
        "            best_comb = [int(max_K/2),1e-4,1,10]\n",
        "        else:\n",
        "            # search space\n",
        "            K_list = range(0, max_K, int(max_K/5))[1:]\n",
        "            lr_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
        "            lrz_list = [10, 1, 1e-1, 1e-2]\n",
        "            C_list = [0.1, 1, 10, 100]\n",
        "            max_num = len(K_list) * len(lr_list) * len(lrz_list) * len(C_list)\n",
        "            comb = random.sample(set(itertools.product(K_list, lr_list, lrz_list, C_list)), max_num)\n",
        "\n",
        "            best_idx = -1\n",
        "            best_acc = -1\n",
        "            for idx in range(min(comb_num,max_num)):\n",
        "                # validating in the tuning dataset if provided\n",
        "                if X_tune != None:\n",
        "                    if verbose == True and idx == 0:\n",
        "                        print('Using validation dataset...',flush=True)\n",
        "                    tmp_acc,_ = run(X_train,y_train,comb[idx],X_tune=X_tune,y_tune=y_tune,important=important,top_rate=top_rate,edged=edged, vWeightd=vWeightd, l1=l1,\n",
        "            epochs=epochs, plot=plot, device=device)\n",
        "                    \n",
        "                # k-fold cross validation if no tuning dataset\n",
        "                else:\n",
        "                    if verbose == True and idx == 0:\n",
        "                        print('Using {}-fold cross validation...'.format(fold),flush=True)\n",
        "\n",
        "                    fold_acc = []\n",
        "                    for train_index, tune_index in StratifiedKFold(n_splits=fold, shuffle = True).split(X_train[0],y_train):\n",
        "                        tmp_X_train = []\n",
        "                        tmp_X_tune = []\n",
        "                        tmp_y_train = y_train[train_index]\n",
        "                        tmp_y_tune = y_train[tune_index]\n",
        "                        for d in range(len(X_train)):\n",
        "                            tmp_X_train.append(X_train[d][train_index,:])\n",
        "                            tmp_X_tune.append(X_train[d][tune_index,:])\n",
        "                        tmp_fold_acc,_ = run(tmp_X_train,tmp_y_train,comb[idx],X_tune=tmp_X_tune,y_tune=tmp_y_tune,important=important,top_rate=top_rate,edged=edged, vWeightd=vWeightd, l1=l1,\n",
        "                epochs=epochs, plot=plot, device=device)\n",
        "                        fold_acc.append(tmp_fold_acc)\n",
        "                    tmp_acc = np.mean(fold_acc)\n",
        "\n",
        "                # update the best hyper-parameter combination\n",
        "                if tmp_acc > best_acc:\n",
        "                    best_idx = idx\n",
        "                    best_acc = tmp_acc\n",
        "            best_comb = comb[best_idx]\n",
        "    \n",
        "    if verbose:\n",
        "        print('*****************************')\n",
        "        print('Best combination of hyper-parameters:')\n",
        "        print('K:{}, lr:{}, lrz:{}, C:{}'.format(comb[best_idx][0],comb[best_idx][1],comb[best_idx][2],comb[best_idx][3]))\n",
        "        print('*****************************')\n",
        "        print()\n",
        "    features, model_2, clf, features_metrics = run(X_train,y_train,best_comb,important=important,top_rate=top_rate,edged=edged, vWeightd=vWeightd, l1=l1,\n",
        "        epochs=epochs, plot=plot, device=device)\n",
        "    return best_comb, features, model_2, clf, features_metrics"
      ],
      "metadata": {
        "id": "X5mbiYOLOZ0U"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(X_train, y_train, X_tune, comb, features,model_2, clf, y_tune=None, top_rate=0.1, edged=None, vWeightd=None, l1=0.1, epochs=1000, plot=False, verbose=False, gpu=False):\n",
        "    \n",
        "    # choose to use GPU or CPU\n",
        "    if gpu == True:\n",
        "        torch.backends.cudnn.enabled = True\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        device = 'cuda'\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "        if verbose:\n",
        "            print(\"Testing using GPU\",flush=True)\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "        if verbose:\n",
        "            print(\"Testing using CPU\",flush=True)\n",
        "\n",
        "    # hyper-parameters\n",
        "    K = comb[0]\n",
        "    lr_net = comb[1]\n",
        "    lr_z = comb[2]\n",
        "    C = comb[3]\n",
        "\n",
        "    D = len(X_train)\n",
        "    n = X_train[0].shape[0]\n",
        "    n_tune = X_tune[0].shape[0]\n",
        "\n",
        "    X_train_normalized = []\n",
        "    X_tune_normalized = []\n",
        "    p = []\n",
        "    for i in range(D):\n",
        "        X_train_normalized.append(torch.tensor(myNormalize(X_train[i])).to(device))\n",
        "        X_tune_normalized.append(torch.tensor(myNormalize(X_tune[i], X_train[i])).to(device))\n",
        "        p.append(X_train[i].shape[1])\n",
        "\n",
        "    if edged != None:\n",
        "        lap = normalized_Laplacian(X_train_normalized, edged, vWeightd)\n",
        "\n",
        "    def norm_21(x):\n",
        "        return torch.norm(x, dim=0).sum()\n",
        "\n",
        "    def unsupervised_loss_1(X, Z_pred, model):\n",
        "        loss = 0\n",
        "        if edged != None:\n",
        "            for i in range(D):\n",
        "                loss += norm_21(X[i] - model[i](Z_pred)) + l1 * norm_21(model[i](Z_pred) @ lap[i].to(device))\n",
        "        else:\n",
        "            for i in range(D):\n",
        "                loss += norm_21(X[i] - model[i](Z_pred)) + l1 * norm_21(model[i](Z_pred))\n",
        "        return loss / X[0].shape[0]\n",
        "\n",
        "    def unsupervised_loss_2(X, Z_pred, model):\n",
        "        loss = 0\n",
        "        for i in range(D):\n",
        "            loss += torch.norm(X[i] - model[i](Z_pred)) ** 2\n",
        "        return loss / X[0].shape[0]\n",
        "\n",
        "    X_tune_fs = []\n",
        "    for d in range(D):\n",
        "        X_tune_fs.append(X_tune_normalized[d][:, features[d]])\n",
        "\n",
        "    # get Z from testing data\n",
        "    Z_tune = torch.randn(n_tune, K, requires_grad=True, device=device)\n",
        "    optimizer_z_tune = torch.optim.Adam([Z_tune], lr_z)\n",
        "    Z_history = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "        loss_z = unsupervised_loss_2(X_tune_fs, Z_tune, model_2)\n",
        "        Z_history.append(loss_z.item())\n",
        "        optimizer_z_tune.zero_grad()\n",
        "        loss_z.backward()\n",
        "        optimizer_z_tune.step()\n",
        "        # projected gradient descent (PGD)\n",
        "        with torch.no_grad():\n",
        "            tmp_norm = torch.norm(Z_tune, dim=1).reshape(-1, 1)\n",
        "            tmp_norm = torch.clamp(tmp_norm, min = 1)\n",
        "            Z_tune.data = Z_tune.data / tmp_norm\n",
        "    if plot:\n",
        "        plt.plot(Z_history)\n",
        "        plt.title(\"Z History\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    if gpu == True:\n",
        "        prediction = clf.predict(Z_tune.cpu().detach().numpy())\n",
        "    else:\n",
        "        prediction = clf.predict(Z_tune.detach().numpy())\n",
        "\n",
        "    if y_tune is not None:\n",
        "        acc = (prediction.reshape(-1) == y_tune).sum().item() / n_tune\n",
        "        return acc\n",
        "    else:\n",
        "        return prediction"
      ],
      "metadata": {
        "id": "Em68_PtPxknj"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_version():\n",
        "    torch_seed(1000)\n",
        "    gt_features = [range(20),range(20)]\n",
        "    epochs=1000\n",
        "    verbose=True\n",
        "    gpu=True\n",
        "\n",
        "    # search times, if 0 then use the default hyper-parameter combination\n",
        "    search_times = 1\n",
        "\n",
        "    # given X_train, y_train, X_tune, and y_tune\n",
        "    best_comb, features, model_2, clf, metrics = train(X_train,y_train,X_tune,y_tune,comb_num=search_times,important=gt_features,epochs=epochs,verbose=verbose,gpu=gpu)\n",
        "    acc= test(X_train,y_train,X_test,best_comb,features,model_2,clf,y_tune=y_test,epochs=epochs,verbose=verbose,gpu=gpu)\n",
        "    print('Accuracy:{}'.format(acc))\n",
        "    print('Evaludation of selected features:')\n",
        "    for i in range(len(metrics)):\n",
        "        print('View {} --> TPR:{}, FPR:{}, F:{}'.format(i+1,metrics[i][0],metrics[i][1],metrics[i][2]))"
      ],
      "metadata": {
        "id": "bnf0GEMUOJeG"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_version()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0aujRZch0mh",
        "outputId": "add85aa3-bcc6-4836-e9ff-f02c33e716c1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training using GPU\n",
            "Using validation dataset...\n",
            "*****************************\n",
            "Best combination of hyper-parameters:\n",
            "K:12, lr:0.1, lrz:0.1, C:100\n",
            "*****************************\n",
            "\n",
            "Testing using GPU\n",
            "Accuracy:0.9944444444444445\n",
            "Evaludation of selected features:\n",
            "View 1 --> TPR:1.0, FPR:0.0, F:1.0\n",
            "View 2 --> TPR:1.0, FPR:0.0, F:1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validation_version():\n",
        "    torch_seed(1000)\n",
        "    gt_features = [range(20),range(20)]\n",
        "    epochs=1000\n",
        "    verbose=True\n",
        "    gpu=True\n",
        "    fold=5\n",
        "    # search times, if 0 then use the default hyper-parameter combination\n",
        "    search_times = 1\n",
        "\n",
        "    # only given X_train, y_train (no X_tune and y_tune)\n",
        "    best_comb, features, model_2, clf, metrics = train(X_train,y_train,comb_num=search_times,important=gt_features,epochs=epochs,fold=fold,verbose=verbose,gpu=gpu)\n",
        "    acc= test(X_train,y_train,X_test,best_comb,features,model_2,clf,y_tune=y_test,epochs=epochs,verbose=verbose,gpu=gpu)\n",
        "    print('Accuracy:{}'.format(acc))\n",
        "    print('Evaludation of selected features:')\n",
        "    for i in range(len(metrics)):\n",
        "        print('View {} --> TPR:{}, FPR:{}, F:{}'.format(i+1,metrics[i][0],metrics[i][1],metrics[i][2]))"
      ],
      "metadata": {
        "id": "xUApG9jchJs6"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_validation_version()"
      ],
      "metadata": {
        "id": "8fTDyWoJYK85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cefa37f-c31d-4623-9e80-09b654cb94f8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training using GPU\n",
            "Using 5-fold cross validation...\n",
            "*****************************\n",
            "Best combination of hyper-parameters:\n",
            "K:12, lr:0.1, lrz:0.1, C:100\n",
            "*****************************\n",
            "\n",
            "Testing using GPU\n",
            "Accuracy:0.9805555555555555\n",
            "Evaludation of selected features:\n",
            "View 1 --> TPR:1.0, FPR:0.0, F:1.0\n",
            "View 2 --> TPR:1.0, FPR:0.0, F:1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UesJvYZi3Uhn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}